--- mm/page_alloc.c
+++ mm/page_alloc.c
@@ -57,7 +57,6 @@
 #include <linux/ftrace_event.h>
 #include <linux/memcontrol.h>
 #include <linux/prefetch.h>
-#include <linux/locallock.h>
 
 #include <asm/tlbflush.h>
 #include <asm/div64.h>
@@ -223,18 +222,6 @@
 EXPORT_SYMBOL(nr_online_nodes);
 #endif
 
-static DEFINE_LOCAL_IRQ_LOCK(pa_lock);
-
-#ifdef CONFIG_PREEMPT_RT_BASE
-# define cpu_lock_irqsave(cpu, flags)		\
-	local_lock_irqsave_on(pa_lock, flags, cpu)
-# define cpu_unlock_irqrestore(cpu, flags)		\
-	local_unlock_irqrestore_on(pa_lock, flags, cpu)
-#else
-# define cpu_lock_irqsave(cpu, flags)		local_irq_save(flags)
-# define cpu_unlock_irqrestore(cpu, flags)	local_irq_restore(flags)
-#endif
-
 int page_group_by_mobility_disabled __read_mostly;
 
 static void set_pageblock_migratetype(struct page *page, int migratetype)
@@ -593,7 +580,7 @@
 }
 
 /*
- * Frees a number of pages which have been collected from the pcp lists.
+ * Frees a number of pages from the PCP lists
  * Assumes all pages on list are in same zone, and of same order.
  * count is the number of pages to free.
  *
@@ -604,42 +591,16 @@
  * pinned" detection logic.
  */
 static void free_pcppages_bulk(struct zone *zone, int count,
-			       struct list_head *list)
+					struct per_cpu_pages *pcp)
 {
+	int migratetype = 0;
+	int batch_free = 0;
 	int to_free = count;
-	unsigned long flags;
 
-	spin_lock_irqsave(&zone->lock, flags);
+	spin_lock(&zone->lock);
 	zone->all_unreclaimable = 0;
 	zone->pages_scanned = 0;
 
-	while (!list_empty(list)) {
-		struct page *page = list_first_entry(list, struct page, lru);
-
-		/* must delete as __free_one_page list manipulates */
-		list_del(&page->lru);
-		/* MIGRATE_MOVABLE list may include MIGRATE_RESERVEs */
-		__free_one_page(page, zone, 0, page_private(page));
-		trace_mm_page_pcpu_drain(page, 0, page_private(page));
-		to_free--;
-	}
-	WARN_ON(to_free != 0);
-	__mod_zone_page_state(zone, NR_FREE_PAGES, count);
-	spin_unlock_irqrestore(&zone->lock, flags);
-}
-
-/*
- * Moves a number of pages from the PCP lists to free list which
- * is freed outside of the locked region.
- *
- * Assumes all pages on list are in same zone, and of same order.
- * count is the number of pages to free.
- */
-static void isolate_pcp_pages(int to_free, struct per_cpu_pages *src,
-			      struct list_head *dst)
-{
-	int migratetype = 0, batch_free = 0;
-
 	while (to_free) {
 		struct page *page;
 		struct list_head *list;
@@ -655,7 +616,7 @@
 			batch_free++;
 			if (++migratetype == MIGRATE_PCPTYPES)
 				migratetype = 0;
-			list = &src->lists[migratetype];
+			list = &pcp->lists[migratetype];
 		} while (list_empty(list));
 
 		/* This is the only non-empty list. Free them all. */
@@ -663,25 +624,28 @@
 			batch_free = to_free;
 
 		do {
-			page = list_last_entry(list, struct page, lru);
+			page = list_entry(list->prev, struct page, lru);
+			/* must delete as __free_one_page list manipulates */
 			list_del(&page->lru);
-			list_add(&page->lru, dst);
+			/* MIGRATE_MOVABLE list may include MIGRATE_RESERVEs */
+			__free_one_page(page, zone, 0, page_private(page));
+			trace_mm_page_pcpu_drain(page, 0, page_private(page));
 		} while (--to_free && --batch_free && !list_empty(list));
 	}
+	__mod_zone_page_state(zone, NR_FREE_PAGES, count);
+	spin_unlock(&zone->lock);
 }
 
 static void free_one_page(struct zone *zone, struct page *page, int order,
 				int migratetype)
 {
-	unsigned long flags;
-
-	spin_lock_irqsave(&zone->lock, flags);
+	spin_lock(&zone->lock);
 	zone->all_unreclaimable = 0;
 	zone->pages_scanned = 0;
 
 	__free_one_page(page, zone, order, migratetype);
 	__mod_zone_page_state(zone, NR_FREE_PAGES, 1 << order);
-	spin_unlock_irqrestore(&zone->lock, flags);
+	spin_unlock(&zone->lock);
 }
 
 static bool free_pages_prepare(struct page *page, unsigned int order)
@@ -718,13 +682,13 @@
 	if (!free_pages_prepare(page, order))
 		return;
 
-	local_lock_irqsave(pa_lock, flags);
+	local_irq_save(flags);
 	if (unlikely(wasMlocked))
 		free_page_mlock(page);
 	__count_vm_events(PGFREE, 1 << order);
 	free_one_page(page_zone(page), page, order,
 					get_pageblock_migratetype(page));
-	local_unlock_irqrestore(pa_lock, flags);
+	local_irq_restore(flags);
 }
 
 /*
@@ -1100,18 +1064,16 @@
 void drain_zone_pages(struct zone *zone, struct per_cpu_pages *pcp)
 {
 	unsigned long flags;
-	LIST_HEAD(dst);
 	int to_drain;
 
-	local_lock_irqsave(pa_lock, flags);
+	local_irq_save(flags);
 	if (pcp->count >= pcp->batch)
 		to_drain = pcp->batch;
 	else
 		to_drain = pcp->count;
-	isolate_pcp_pages(to_drain, pcp, &dst);
+	free_pcppages_bulk(zone, to_drain, pcp);
 	pcp->count -= to_drain;
-	local_unlock_irqrestore(pa_lock, flags);
-	free_pcppages_bulk(zone, to_drain, &dst);
+	local_irq_restore(flags);
 }
 #endif
 
@@ -1130,21 +1092,16 @@
 	for_each_populated_zone(zone) {
 		struct per_cpu_pageset *pset;
 		struct per_cpu_pages *pcp;
-		LIST_HEAD(dst);
-		int count;
 
-		cpu_lock_irqsave(cpu, flags);
+		local_irq_save(flags);
 		pset = per_cpu_ptr(zone->pageset, cpu);
 
 		pcp = &pset->pcp;
-		count = pcp->count;
-		if (count) {
-			isolate_pcp_pages(count, pcp, &dst);
+		if (pcp->count) {
+			free_pcppages_bulk(zone, pcp->count, pcp);
 			pcp->count = 0;
 		}
-		cpu_unlock_irqrestore(cpu, flags);
-		if (count)
-			free_pcppages_bulk(zone, count, &dst);
+		local_irq_restore(flags);
 	}
 }
 
@@ -1161,14 +1118,7 @@
  */
 void drain_all_pages(void)
 {
-#ifndef CONFIG_PREEMPT_RT_BASE
 	on_each_cpu(drain_local_pages, NULL, 1);
-#else
-	int i;
-
-	for_each_online_cpu(i)
-		drain_pages(i);
-#endif
 }
 
 #ifdef CONFIG_HIBERNATION
@@ -1224,7 +1174,7 @@
 
 	migratetype = get_pageblock_migratetype(page);
 	set_page_private(page, migratetype);
-	local_lock_irqsave(pa_lock, flags);
+	local_irq_save(flags);
 	if (unlikely(wasMlocked))
 		free_page_mlock(page);
 	__count_vm_event(PGFREE);
@@ -1251,19 +1201,12 @@
 		list_add(&page->lru, &pcp->lists[migratetype]);
 	pcp->count++;
 	if (pcp->count >= pcp->high) {
-		LIST_HEAD(dst);
-		int count;
-
-		isolate_pcp_pages(pcp->batch, pcp, &dst);
+		free_pcppages_bulk(zone, pcp->batch, pcp);
 		pcp->count -= pcp->batch;
-		count = pcp->batch;
-		local_unlock_irqrestore(pa_lock, flags);
-		free_pcppages_bulk(zone, count, &dst);
-		return;
 	}
 
 out:
-	local_unlock_irqrestore(pa_lock, flags);
+	local_irq_restore(flags);
 }
 
 /*
@@ -1358,7 +1301,7 @@
 		struct per_cpu_pages *pcp;
 		struct list_head *list;
 
-		local_lock_irqsave(pa_lock, flags);
+		local_irq_save(flags);
 		pcp = &this_cpu_ptr(zone->pageset)->pcp;
 		list = &pcp->lists[migratetype];
 		if (list_empty(list)) {
@@ -1390,19 +1333,17 @@
 			 */
 			WARN_ON_ONCE(order > 1);
 		}
-		local_spin_lock_irqsave(pa_lock, &zone->lock, flags);
+		spin_lock_irqsave(&zone->lock, flags);
 		page = __rmqueue(zone, order, migratetype);
-		if (!page) {
-			spin_unlock(&zone->lock);
+		spin_unlock(&zone->lock);
+		if (!page)
 			goto failed;
-		}
 		__mod_zone_page_state(zone, NR_FREE_PAGES, -(1 << order));
-		spin_unlock(&zone->lock);
 	}
 
 	__count_zone_vm_events(PGALLOC, zone, 1 << order);
 	zone_statistics(preferred_zone, zone, gfp_flags);
-	local_unlock_irqrestore(pa_lock, flags);
+	local_irq_restore(flags);
 
 	VM_BUG_ON(bad_range(zone, page));
 	if (prep_new_page(page, order, gfp_flags))
@@ -1410,7 +1351,7 @@
 	return page;
 
 failed:
-	local_unlock_irqrestore(pa_lock, flags);
+	local_irq_restore(flags);
 	return NULL;
 }
 
@@ -1977,8 +1918,8 @@
 	if (*did_some_progress != COMPACT_SKIPPED) {
 
 		/* Page migration frees to the PCP lists but we want merging */
-		drain_pages(get_cpu_light());
-		put_cpu_light();
+		drain_pages(get_cpu());
+		put_cpu();
 
 		page = get_page_from_freelist(gfp_mask, nodemask,
 				order, zonelist, high_zoneidx,
@@ -3796,16 +3737,14 @@
 	for_each_possible_cpu(cpu) {
 		struct per_cpu_pageset *pset;
 		struct per_cpu_pages *pcp;
-		LIST_HEAD(dst);
 
 		pset = per_cpu_ptr(zone->pageset, cpu);
 		pcp = &pset->pcp;
 
-		cpu_lock_irqsave(cpu, flags);
-		isolate_pcp_pages(pcp->count, pcp, &dst);
-		free_pcppages_bulk(zone, pcp->count, &dst);
+		local_irq_save(flags);
+		free_pcppages_bulk(zone, pcp->count, pcp);
 		setup_pageset(pset, batch);
-		cpu_unlock_irqrestore(cpu, flags);
+		local_irq_restore(flags);
 	}
 	return 0;
 }
@@ -5120,7 +5059,6 @@
 void __init page_alloc_init(void)
 {
 	hotcpu_notifier(page_alloc_cpu_notify, 0);
-	local_irq_lock_init(pa_lock);
 }
 
 /*
